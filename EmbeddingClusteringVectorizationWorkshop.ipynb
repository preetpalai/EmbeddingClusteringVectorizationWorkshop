{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "832c238b",
   "metadata": {},
   "source": [
    "# Embedding Clustering and Vectorization Workshop\n",
    "\n",
    "This notebook demonstrates the implementation of Word2Vec and GloVe embedding models on a real-world text corpus relevant to our final project.\n",
    "\n",
    "## Team Members\n",
    "- Kapil\n",
    "- Parag\n",
    "- Preetpal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9429cba3",
   "metadata": {},
   "source": [
    "## üîÑ NLP Preprocessing Pipeline\n",
    "Steps:\n",
    "1. Document collection\n",
    "2. Tokenization\n",
    "3. Lowercasing\n",
    "4. Stopword removal\n",
    "5. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f1e1351",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # This line ensures punkt is available\n",
    "nltk.download('stopwords')  # Already done, but safe to call again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfc420dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['titanic', 'massive', 'passenger', 'liner', 'sank', 'north', 'atlantic', 'ocean', 'hitting', 'iceberg', 'people', 'died', 'one', 'deadliest', 'commercial', 'peacetime', 'maritime', 'disasters', 'modern', 'history', 'titanic', 'considered', 'unsinkable', 'nature', 'proved', 'otherwise', 'century', 'later', 'submersible', 'named', 'titan', 'imploded', 'descending', 'titanic', 'wreck', 'site', 'titan', 'privatelyoperated', 'deepsea', 'vehicle', 'designed', 'underwater', 'exploration', 'tragically', 'five', 'people', 'onboard', 'killed', 'titans', 'disappearance']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the file\n",
    "with open(\"./Data/Corpus_Titanic.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_corpus = file.read()\n",
    "\n",
    "# Step 2: Import libraries\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Step 3: Download and set up NLTK resources\n",
    "nltk_data_dir = \"nltk_data\"  # local folder\n",
    "nltk.download('punkt', download_dir=nltk_data_dir)\n",
    "nltk.download('stopwords', download_dir=nltk_data_dir)\n",
    "\n",
    "# Add to NLTK path explicitly\n",
    "nltk.data.path.append(nltk_data_dir)\n",
    "\n",
    "# Step 4: Preprocessing function\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # remove punctuation/numbers\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "# Step 5: Apply preprocessing\n",
    "tokens = preprocess(raw_corpus)\n",
    "\n",
    "# Step 6: Print output\n",
    "print(tokens[:50])  # show first 50 tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b02a4db",
   "metadata": {},
   "source": [
    "## üî§ Word2Vec Implementation (Predictive Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9951d7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Top 10 words similar to 'titanic':\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Word",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Similarity Score",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "3f0448d7-7d57-40f3-ab30-52518bbafd0e",
       "rows": [
        [
         "0",
         "sea",
         "0.2714272737503052"
        ],
        [
         "1",
         "descending",
         "0.27121418714523315"
        ],
        [
         "2",
         "raised",
         "0.2695865333080292"
        ],
        [
         "3",
         "peacetime",
         "0.2542565166950226"
        ],
        [
         "4",
         "passenger",
         "0.24141450226306915"
        ],
        [
         "5",
         "ended",
         "0.2396603524684906"
        ],
        [
         "6",
         "investigators",
         "0.22448116540908813"
        ],
        [
         "7",
         "site",
         "0.2110331803560257"
        ],
        [
         "8",
         "private",
         "0.19652478396892548"
        ],
        [
         "9",
         "killed",
         "0.1865294873714447"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Similarity Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sea</td>\n",
       "      <td>0.271427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>descending</td>\n",
       "      <td>0.271214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>raised</td>\n",
       "      <td>0.269587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>peacetime</td>\n",
       "      <td>0.254257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>passenger</td>\n",
       "      <td>0.241415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ended</td>\n",
       "      <td>0.239660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>investigators</td>\n",
       "      <td>0.224481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>site</td>\n",
       "      <td>0.211033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>private</td>\n",
       "      <td>0.196525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>killed</td>\n",
       "      <td>0.186529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  Similarity Score\n",
       "0            sea          0.271427\n",
       "1     descending          0.271214\n",
       "2         raised          0.269587\n",
       "3      peacetime          0.254257\n",
       "4      passenger          0.241415\n",
       "5          ended          0.239660\n",
       "6  investigators          0.224481\n",
       "7           site          0.211033\n",
       "8        private          0.196525\n",
       "9         killed          0.186529"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Load the corpus\n",
    "with open(\"Data/Corpus_Titanic.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_corpus = file.read()\n",
    "\n",
    "# Step 2: Import required libraries\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Step 3: Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 4: Preprocess function (sentence and word level)\n",
    "def preprocess_sentences(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # remove punctuation and digits\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokenized = [\n",
    "        [word for word in word_tokenize(sent) if word not in stop_words]\n",
    "        for sent in sentences\n",
    "    ]\n",
    "    return tokenized\n",
    "\n",
    "# Step 5: Tokenize the corpus\n",
    "tokenized_docs = preprocess_sentences(raw_corpus)\n",
    "\n",
    "# Step 6: Train Word2Vec model\n",
    "w2v_model = Word2Vec(sentences=tokenized_docs, vector_size=50, window=3, min_count=1, workers=4)\n",
    "w2v_model.save(\"word2vec.model\")\n",
    "\n",
    "# Step 7: Get similar words to \"titanic\"\n",
    "try:\n",
    "    similar_words = w2v_model.wv.most_similar('titanic', topn=10)\n",
    "except KeyError:\n",
    "    similar_words = []\n",
    "\n",
    "# Step 8: Display results in grid format using pandas\n",
    "if similar_words:\n",
    "    df_similar = pd.DataFrame(similar_words, columns=['Word', 'Similarity Score'])\n",
    "    print(\"üîç Top 10 words similar to 'titanic':\")\n",
    "    display(df_similar)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è The word 'titanic' was not found in the vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6148b590",
   "metadata": {},
   "source": [
    "## üìä GloVe Implementation (Count-based Model)\n",
    "Note: We'll use the `glove_python` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdb9613c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
      "Similar words to 'titanic' using pretrained GloVe:\n",
      "odyssey: 0.6530\n",
      "phantom: 0.6510\n",
      "doomed: 0.6414\n",
      "r.m.s.: 0.6303\n",
      "cinderella: 0.6263\n",
      "voyager: 0.6227\n",
      "wreck: 0.6045\n",
      "ghost: 0.5991\n",
      "horror: 0.5960\n",
      "tragedy: 0.5954\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load pretrained GloVe 50-dimensional embeddings (takes a while first time)\n",
    "glove_vectors = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "# Find similar words to 'titanic'\n",
    "similar_words = glove_vectors.most_similar('titanic', topn=10)\n",
    "\n",
    "print(\"Similar words to 'titanic' using pretrained GloVe:\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cdf6bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting glove-python\n",
      "  Using cached glove_python-0.1.0.tar.gz (263 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from glove-python) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\acer\\appdata\\roaming\\python\\python312\\site-packages (from glove-python) (1.13.1)\n",
      "Building wheels for collected packages: glove-python\n",
      "  Building wheel for glove-python (setup.py): started\n",
      "  Building wheel for glove-python (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for glove-python\n",
      "Failed to build glove-python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'glove-python' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'glove-python'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  √ó python setup.py bdist_wheel did not run successfully.\n",
      "  ‚îÇ exit code: 1\n",
      "  ‚ï∞‚îÄ> [68 lines of output]\n",
      "      C:\\Users\\acer\\AppData\\Local\\Temp\\pip-install-_yvtqf4k\\glove-python_32f0b5f39ef245ce9aa963d9ebe5fa2c\\setup.py:8: SetuptoolsDeprecationWarning: The test command is disabled and references to it are deprecated.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Please remove any references to `setuptools.command.test` in all supported versions of the affected package.\n",
      "      \n",
      "              This deprecation is overdue, please update your project and remove deprecated\n",
      "              calls to avoid build errors in the future.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        from setuptools.command.test import test as TestCommand\n",
      "      C:\\Users\\acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\setuptools\\dist.py:601: SetuptoolsDeprecationWarning: Invalid dash-separated key 'description-file' in 'metadata' (setup.cfg), please use the underscore name 'description_file' instead.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Usage of dash-separated 'description-file' will not be supported in future\n",
      "              versions. Please use the underscore name 'description_file' instead.\n",
      "      \n",
      "              By 2026-Mar-03, you need to update your project and remove deprecated calls\n",
      "              or your builds will no longer be supported.\n",
      "      \n",
      "              See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        opt = self._enforce_underscore(opt, section)\n",
      "      C:\\Users\\acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\setuptools\\_distutils\\dist.py:289: UserWarning: Unknown distribution option: 'tests_require'\n",
      "        warnings.warn(msg)\n",
      "      C:\\Users\\acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\setuptools\\dist.py:601: SetuptoolsDeprecationWarning: Invalid dash-separated key 'description-file' in 'metadata' (setup.cfg), please use the underscore name 'description_file' instead.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Usage of dash-separated 'description-file' will not be supported in future\n",
      "              versions. Please use the underscore name 'description_file' instead.\n",
      "              (Affected: glove_python).\n",
      "      \n",
      "              By 2026-Mar-03, you need to update your project and remove deprecated calls\n",
      "              or your builds will no longer be supported.\n",
      "      \n",
      "              See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        opt = self._enforce_underscore(opt, section)\n",
      "      C:\\Users\\acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\setuptools\\dist.py:761: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "      \n",
      "              License :: OSI Approved :: Apache Software License\n",
      "      \n",
      "              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        self._finalize_license_expression()\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\\lib.win-amd64-cpython-312\\glove\n",
      "      copying glove\\corpus.py -> build\\lib.win-amd64-cpython-312\\glove\n",
      "      copying glove\\glove.py -> build\\lib.win-amd64-cpython-312\\glove\n",
      "      copying glove\\__init__.py -> build\\lib.win-amd64-cpython-312\\glove\n",
      "      running build_ext\n",
      "      building 'glove.glove_cython' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for glove-python\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  √ó python setup.py clean did not run successfully.\n",
      "  ‚îÇ exit code: 1\n",
      "  ‚ï∞‚îÄ> [64 lines of output]\n",
      "      C:\\Users\\acer\\AppData\\Local\\Temp\\pip-install-_yvtqf4k\\glove-python_32f0b5f39ef245ce9aa963d9ebe5fa2c\\setup.py:8: SetuptoolsDeprecationWarning: The test command is disabled and references to it are deprecated.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Please remove any references to `setuptools.command.test` in all supported versions of the affected package.\n",
      "      \n",
      "              This deprecation is overdue, please update your project and remove deprecated\n",
      "              calls to avoid build errors in the future.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        from setuptools.command.test import test as TestCommand\n",
      "      C:\\Users\\acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\setuptools\\dist.py:601: SetuptoolsDeprecationWarning: Invalid dash-separated key 'description-file' in 'metadata' (setup.cfg), please use the underscore name 'description_file' instead.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Usage of dash-separated 'description-file' will not be supported in future\n",
      "              versions. Please use the underscore name 'description_file' instead.\n",
      "      \n",
      "              By 2026-Mar-03, you need to update your project and remove deprecated calls\n",
      "              or your builds will no longer be supported.\n",
      "      \n",
      "              See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        opt = self._enforce_underscore(opt, section)\n",
      "      C:\\Users\\acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\setuptools\\_distutils\\dist.py:289: UserWarning: Unknown distribution option: 'tests_require'\n",
      "        warnings.warn(msg)\n",
      "      C:\\Users\\acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\setuptools\\dist.py:601: SetuptoolsDeprecationWarning: Invalid dash-separated key 'description-file' in 'metadata' (setup.cfg), please use the underscore name 'description_file' instead.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Usage of dash-separated 'description-file' will not be supported in future\n",
      "              versions. Please use the underscore name 'description_file' instead.\n",
      "              (Affected: glove_python).\n",
      "      \n",
      "              By 2026-Mar-03, you need to update your project and remove deprecated calls\n",
      "              or your builds will no longer be supported.\n",
      "      \n",
      "              See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        opt = self._enforce_underscore(opt, section)\n",
      "      C:\\Users\\acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\setuptools\\dist.py:761: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "      \n",
      "              License :: OSI Approved :: Apache Software License\n",
      "      \n",
      "              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        self._finalize_license_expression()\n",
      "      usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n",
      "         or: setup.py --help [cmd1 cmd2 ...]\n",
      "         or: setup.py --help-commands\n",
      "         or: setup.py cmd --help\n",
      "      \n",
      "      error: option --all not recognized\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed cleaning build dir for glove-python\n",
      "ERROR: Failed to build installable wheels for some pyproject.toml based projects (glove-python)\n"
     ]
    }
   ],
   "source": [
    "!pip install glove-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1303d4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1924384277.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfrom glove import Corpus, Glove\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "    from glove import Corpus, Glove\n",
    "\n",
    "    # Step 1: Build the corpus\n",
    "    corpus = Corpus()\n",
    "    corpus.fit(tokenized_docs, window=3)\n",
    "\n",
    "    # Step 2: Train the GloVe model\n",
    "    glove = Glove(no_components=50, learning_rate=0.05)\n",
    "    glove.fit(corpus.matrix, epochs=10, no_threads=4, verbose=True)\n",
    "    glove.add_dictionary(corpus.dictionary)\n",
    "\n",
    "    # Step 3: Get similar words to \"titanic\"\n",
    "    similar_glove_words = glove.most_similar('titanic', number=10)\n",
    "\n",
    "    # Step 4: Display results in a table\n",
    "    import pandas as pd\n",
    "\n",
    "    df_glove = pd.DataFrame(similar_glove_words, columns=[\"Word\", \"Similarity Score\"])\n",
    "    print(\"üîç Top 10 words similar to 'titanic' using GloVe:\")\n",
    "    display(df_glove)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58f76c3",
   "metadata": {},
   "source": [
    "## üìà Talking Points: Word2Vec vs. GloVe\n",
    "\n",
    "| Feature | Word2Vec | GloVe |\n",
    "|--------|----------|--------|\n",
    "| Model Type | Predictive | Count-based |\n",
    "| Training Speed | Slower due to context prediction | Faster with precomputed co-occurrence matrix |\n",
    "| Handles Rare Words | Less effective | Better representation of rare words |\n",
    "| Semantic Accuracy | High | Moderate |\n",
    "| Use Case Fit | Better for dynamic language (e.g., user reviews) | Better for static corpora like product catalogs |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526d8ffd",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "- Implemented both predictive and count-based embedding models\n",
    "- Cleaned and tokenized real-world documents from our Grocery Buddies project\n",
    "- Compared GloVe vs Word2Vec in practical use cases\n",
    "- All members contributed equally to this collaborative peer-reviewed notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
